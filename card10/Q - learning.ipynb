{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4878730ec784f1",
   "metadata": {},
   "source": [
    "# Aprendizagem por Reforço\n",
    "\n",
    "Vamos descrever o \"problema do táxi\". Queremos construir um táxi autônomo que possa pegar passageiros em um conjunto de locais fixos, deixá-los em outro local e chegar lá no menor tempo possível, evitando obstáculos.\n",
    "\n",
    "O AI Gym nos permite criar este ambiente rapidamente:"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:14.229398Z",
     "start_time": "2024-10-07T00:13:48.859913Z"
    }
   },
   "source": [
    "import gym  # Importa a biblioteca gym para ambientes de aprendizado por reforço\n",
    "import random  # Importa a biblioteca random para geração de números aleatórios\n",
    "\n",
    "# Define a semente para a geração de números aleatórios, garantindo reprodutibilidade\n",
    "random.seed(1234)\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Renderiza o ambiente, exibindo a interface gráfica\n",
    "streets.render()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001B[43m \u001B[0m: | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |\u001B[35mB\u001B[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9e549ebf8cb44753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:14.237019Z",
     "start_time": "2024-10-07T00:14:14.231403Z"
    }
   },
   "source": [
    "# Resetando o ambiente para o estado inicial\n",
    "streets.reset()\n",
    "\n",
    "# Loop de exemplo para tomar algumas ações aleatórias\n",
    "for _ in range(1):\n",
    "    action = streets.action_space.sample()  # Toma uma ação aleatória\n",
    "    observation, reward, done, info = streets.step(action)  # Executa a ação\n",
    "    streets.render()  # Renderiza o ambiente\n",
    "    if done:  # Se o episódio terminou\n",
    "        break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001B[43m \u001B[0m| : :\u001B[34;1mG\u001B[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001B[35mB\u001B[0m: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c96f6c58a51ac946",
   "metadata": {},
   "source": [
    "Vamos detalhar o que estamos vendo aqui:\n",
    "\n",
    "- R, G, B e Y são locais de coleta ou entrega.\n",
    "- A letra AZUL indica onde precisamos buscar alguém.\n",
    "- A letra MAGENTA indica para onde aquele passageiro deseja ir.\n",
    "- As linhas sólidas representam paredes que o táxi não pode atravessar.\n",
    "- O retângulo preenchido representa o próprio táxi - é amarelo quando está vazio e verde quando transporta um passageiro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55eae735f08b33",
   "metadata": {},
   "source": [
    "Nosso mundinho aqui, que chamamos de “ruas”, é uma grade 5x5. O estado deste mundo a qualquer momento pode ser definido por:\n",
    "\n",
    "- Onde está o táxi (um de 5x5 = 25 locais)\n",
    "- Qual é o destino atual (4 possibilidades)\n",
    "- Onde o passageiro está (5 possibilidades: em um dos destinos, ou dentro do táxi)\n",
    "\n",
    "Portanto, há um total de 25 x 4 x 5 = 500 estados possíveis que descrevem o nosso mundo.\n",
    "\n",
    "Para cada estado, existem seis ações possíveis:\n",
    "\n",
    "- Mova-se para o sul, leste, norte ou oeste\n",
    "- Pegue um passageiro\n",
    "- Deixar um passageiro\n",
    "\n",
    "O Q-Learning ocorrerá usando as seguintes recompensas e penalidades em cada estado:\n",
    "\n",
    "- Uma entrega bem-sucedida rende +20 pontos\n",
    "- Cada passo dado enquanto dirige um passageiro gera uma penalidade de -1 ponto\n",
    "- Pegar ou deixar em um local ilegal gera uma penalidade de -10 pontos\n",
    "\n",
    "Atravessar uma parede simplesmente não é permitido.\n",
    "\n",
    "Vamos definir um estado inicial, com o táxi no local (2, 3), o passageiro no local de partida 2 e o destino no local 0:"
   ]
  },
  {
   "cell_type": "code",
   "id": "efb6ba8694415234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:14.242985Z",
     "start_time": "2024-10-07T00:14:14.238901Z"
    }
   },
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0)\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "streets.render()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001B[43m \u001B[0m: |\n",
      "| | : | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "eb6de157fb0604e2",
   "metadata": {},
   "source": [
    "Vamos examinar a tabela de recompensas para este estado inicial:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ef943c27249770f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:14.253387Z",
     "start_time": "2024-10-07T00:14:14.244Z"
    }
   },
   "source": [
    "streets.P[initial_state]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "127ff39d372e4591",
   "metadata": {},
   "source": [
    "Veja como interpretar isso - cada linha corresponde a uma ação potencial neste estado: mover-se para Sul, Norte, Leste ou Oeste, embarque ou desembarque. Os quatro valores em cada linha são a probabilidade atribuída a essa ação, o próximo estado resultante dessa ação, a recompensa por essa ação e se essa ação indica que ocorreu uma desistência bem-sucedida. \n",
    "\n",
    "Assim, por exemplo, mudar para o norte deste estado nos colocaria no estado número 368, incorreria em uma penalidade de -1 por ocupar tempo e não resultaria em uma desistência bem-sucedida.\n",
    "\n",
    "Então, vamos fazer Q-learning! Primeiro precisamos treinar nosso modelo. Em alto nível, treinaremos mais de 10.000 corridas simuladas de táxi. Para cada execução, avançaremos no tempo, com 10% de chance em cada etapa de realizar uma etapa exploratória aleatória, em vez de usar os valores Q aprendidos para guiar nossas ações."
   ]
  },
  {
   "cell_type": "code",
   "id": "1635b065e93b4a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:18.687610Z",
     "start_time": "2024-10-07T00:14:14.256393Z"
    }
   },
   "source": [
    "import numpy as np  # Importa a biblioteca NumPy para operações numéricas\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.1  # Taxa de aprendizado\n",
    "discount_factor = 0.6  # Fator de desconto\n",
    "exploration = 0.1  # Probabilidade de explorar ações aleatórias\n",
    "epochs = 10000  # Número de episódios\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    \n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "            \n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "        \n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "        \n",
    "        state = next_state  # Move para o próximo estado\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6acbcc961ac806ce",
   "metadata": {},
   "source": [
    "Portanto, agora temos uma tabela de valores Q que pode ser usada rapidamente para determinar o próximo passo ideal para qualquer estado! Vamos verificar na tabela nosso estado inicial acima:"
   ]
  },
  {
   "cell_type": "code",
   "id": "113b5d0b7d7106a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:14:18.695629Z",
     "start_time": "2024-10-07T00:14:18.688616Z"
    }
   },
   "source": [
    "q_table[initial_state]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.42319771, -2.41011019, -2.4088489 , -2.3639511 , -7.28733341,\n",
       "       -7.33134259])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d32f5ffefa87cfba",
   "metadata": {},
   "source": [
    "O valor q mais baixo aqui corresponde à ação \"ir para oeste\", o que faz sentido - essa é a rota mais direta em direção ao nosso destino a partir desse ponto. Parece funcionar! Vamos ver isso em ação!"
   ]
  },
  {
   "cell_type": "code",
   "id": "cdb29f4d529f228e",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output  # Importa função para limpar a saída do notebook\n",
    "from time import sleep  # Importa função para criar pausas no código\n",
    "\n",
    "# Loop para simular 10 viagens\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    \n",
    "    done = False  # Sinaliza se a viagem terminou\n",
    "    trip_length = 0  # Contador para o comprimento da viagem\n",
    "    \n",
    "    # Loop para executar os passos da viagem, com limite de 25 passos\n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])  # Escolhe a melhor ação com base na Q-table\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "        \n",
    "        clear_output(wait=True)  # Limpa a saída anterior do notebook\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))  # Exibe o número da viagem e o passo atual\n",
    "        print(streets.render(mode='ansi'))  # Renderiza o estado do ambiente em formato ASCII\n",
    "        sleep(0.5)  # Pausa de 0,5 segundos para melhor visualização\n",
    "        state = next_state  # Atualiza o estado atual para o próximo estado\n",
    "        trip_length += 1  # Incrementa o contador do comprimento da viagem\n",
    "        \n",
    "    sleep(2)  # Pausa de 2 segundos antes de iniciar a próxima viagem\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7a7e96fc7f3879eb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table (supondo que já foi treinada)\n",
    "# Você deve ter a Q-table `q_table` já definida a partir do treinamento anterior\n",
    "\n",
    "# Visualização das viagens do agente\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()  # Reinicia o ambiente para uma nova viagem\n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:  # Limita a 25 passos por viagem\n",
    "        action = np.argmax(q_table[state])  # Escolhe a melhor ação com base na Q-table\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação\n",
    "        \n",
    "        clear_output(wait=True)  # Limpa a saída anterior\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))  # Exibe o número da viagem e o passo atual\n",
    "        print(streets.render(mode='ansi'))  # Renderiza o estado do ambiente no modo texto\n",
    "        sleep(0.5)  # Pausa para visualizar o movimento do agente\n",
    "        \n",
    "        state = next_state  # Atualiza o estado para o próximo passo\n",
    "        trip_length += 1  # Incrementa o comprimento da viagem\n",
    "    \n",
    "    sleep(2)  # Pausa entre as viagens\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f728a7b40e7d57c3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.5  # Aumentada para permitir aprendizado mais rápido\n",
    "discount_factor = 0.99  # Aumentado para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.1  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 100000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4819ff183a701ea",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Ajustado para ver se acelera o aprendizado\n",
    "discount_factor = 0.95  # Mantido alto para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 50000  # Aumentado para dar mais tempo de aprendizado\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "398b427752c99cde",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Ajustado para ver se acelera o aprendizado\n",
    "discount_factor = 0.95  # Mantido alto para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Aumentado para dar mais tempo de aprendizado\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83b90c697d1f5e7a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa duas Q-tables com zeros\n",
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Taxa de aprendizado\n",
    "discount_factor = 0.95  # Fator de desconto\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        # Escolha aleatória entre as duas Q-tables\n",
    "        if random.uniform(0, 1) < initial_exploration:\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a tabela escolhida com a nova Q-value\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]  # Valor Q anterior\n",
    "            next_max_q = np.max(q_table_b[next_state])  # Valor Q máximo do próximo estado na tabela B\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "            q_table_a[state, action] = new_q  # Atualiza a Q-table A\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]  # Valor Q anterior\n",
    "            next_max_q = np.max(q_table_a[next_state])  # Valor Q máximo do próximo estado na tabela A\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "            q_table_b[state, action] = new_q  # Atualiza a Q-table B\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e63ee34a501e05fa",
   "metadata": {},
   "source": [
    "## Seu desafio\n",
    "\n",
    "Modifique o bloco acima para acompanhar os intervalos de tempo totais e use isso como uma métrica para avaliar a qualidade do nosso sistema Q-learning. Você pode querer aumentar o número de viagens simuladas e remover as chamadas sleep() para permitir a execução de mais amostras.\n",
    "\n",
    "Agora, tente fazer experiências com os hiperparâmetros. Quão baixo pode chegar o número de épocas antes que nosso modelo comece a sofrer? Você consegue encontrar melhores taxas de aprendizagem, fatores de desconto ou fatores de exploração para tornar o treinamento mais eficiente? A taxa de exploração versus exploração, em particular, é interessante de experimentar."
   ]
  },
  {
   "cell_type": "code",
   "id": "eff815c969c398a3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym  # Importa a biblioteca gym para ambientes de aprendizado por reforço\n",
    "import random  # Importa a biblioteca random para geração de números aleatórios\n",
    "import numpy as np  # Importa a biblioteca numpy para operações numéricas\n",
    "import time  # Importa a biblioteca time para medir intervalos de tempo\n",
    "\n",
    "# Define a semente para a geração de números aleatórios, garantindo reprodutibilidade\n",
    "random.seed(1234)\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Renderiza o ambiente, exibindo a interface gráfica\n",
    "streets.render()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3e8c0146aa2bf79",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Taxa de aprendizado\n",
    "discount_factor = 0.95  # Fator de desconto\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas e tempos de viagem\n",
    "reward_list = []\n",
    "time_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    \n",
    "    done = False  # Sinaliza se a viagem terminou\n",
    "    trip_length = 0  # Contador para o comprimento da viagem\n",
    "    start_time = time.time()  # Marca o tempo inicial da viagem\n",
    "    \n",
    "    while not done and trip_length < 25:  # Limita a 25 passos\n",
    "        action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])  # Escolhe a ação\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação\n",
    "        \n",
    "        # Atualiza a tabela escolhida com a nova Q-value\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]\n",
    "            next_max_q = np.max(q_table_b[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_a[state, action] = new_q\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]\n",
    "            next_max_q = np.max(q_table_a[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_b[state, action] = new_q\n",
    "\n",
    "        state = next_state  # Atualiza o estado atual\n",
    "        trip_length += 1  # Incrementa o contador do comprimento da viagem\n",
    "    \n",
    "    total_time = time.time() - start_time  # Calcula o tempo total da viagem\n",
    "    reward_list.append(reward)  # Armazena a recompensa do episódio\n",
    "    time_list.append(total_time)  # Armazena o tempo da viagem\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas e o tempo médio das viagens\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "print(f\"Tempo médio das viagens: {np.mean(time_list)} segundos\")\n",
    "\n",
    "# Plote as recompensas e tempos ao longo do tempo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.convolve(time_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 viagens)')\n",
    "plt.ylabel('Tempo médio (segundos)')\n",
    "plt.title('Tempo Médio das Viagens em Taxi-v3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a47443bf02bdf589",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurações iniciais\n",
    "random.seed(1234)\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa as Q-tables\n",
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.2  # Taxa de aprendizado ajustada\n",
    "discount_factor = 0.99  # Fator de desconto ajustado\n",
    "initial_exploration = 1.0\n",
    "final_exploration = 0.05\n",
    "decay_rate = 0.995\n",
    "epochs = 500000\n",
    "\n",
    "# Armazenamento de recompensas e tempos\n",
    "reward_list = []\n",
    "time_list = []\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        # Atualiza as Q-tables\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]\n",
    "            next_max_q = np.max(q_table_b[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_a[state, action] = new_q\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]\n",
    "            next_max_q = np.max(q_table_a[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_b[state, action] = new_q\n",
    "\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    reward_list.append(reward)\n",
    "    time_list.append(total_time)\n",
    "\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "print(f\"Tempo médio das viagens: {np.mean(time_list)} segundos\")\n",
    "\n",
    "# Gráficos\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.convolve(time_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 viagens)')\n",
    "plt.ylabel('Tempo médio (segundos)')\n",
    "plt.title('Tempo Médio das Viagens em Taxi-v3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "160454faed0ae242",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# SARSA\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.2\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "final_exploration = 0.05\n",
    "decay_rate = 0.995\n",
    "epochs = 500000\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    action = streets.action_space.sample()  # Ação inicial aleatória\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        # Atualiza a Q-value usando a regra SARSA\n",
    "        next_action = streets.action_space.sample() if random.uniform(0, 1) < initial_exploration else np.argmax(q_table[next_state])\n",
    "        q_table[state, action] += learning_rate * (reward + discount_factor * q_table[next_state, next_action] - q_table[state, action])\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "    reward_list.append(reward)\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Gráficos\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3 com SARSA')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
