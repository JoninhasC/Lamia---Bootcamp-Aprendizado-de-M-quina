{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Aprendizagem por Reforço\n",
    "\n",
    "Vamos descrever o \"problema do táxi\". Queremos construir um táxi autônomo que possa pegar passageiros em um conjunto de locais fixos, deixá-los em outro local e chegar lá no menor tempo possível, evitando obstáculos.\n",
    "\n",
    "O AI Gym nos permite criar este ambiente rapidamente:"
   ],
   "id": "6b4878730ec784f1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:25.698609Z",
     "start_time": "2024-09-23T19:40:25.689360Z"
    }
   },
   "source": [
    "import gym  # Importa a biblioteca gym para ambientes de aprendizado por reforço\n",
    "import random  # Importa a biblioteca random para geração de números aleatórios\n",
    "\n",
    "# Define a semente para a geração de números aleatórios, garantindo reprodutibilidade\n",
    "random.seed(1234)\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Renderiza o ambiente, exibindo a interface gráfica\n",
    "streets.render()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35m\u001B[43mR\u001B[0m\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:25.747058Z",
     "start_time": "2024-09-23T19:40:25.741971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Resetando o ambiente para o estado inicial\n",
    "streets.reset()\n",
    "\n",
    "# Loop de exemplo para tomar algumas ações aleatórias\n",
    "for _ in range(1):\n",
    "    action = streets.action_space.sample()  # Toma uma ação aleatória\n",
    "    observation, reward, done, info = streets.step(action)  # Executa a ação\n",
    "    streets.render()  # Renderiza o ambiente\n",
    "    if done:  # Se o episódio terminou\n",
    "        break\n"
   ],
   "id": "9e549ebf8cb44753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001B[34;1mG\u001B[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35mY\u001B[0m| : |B:\u001B[43m \u001B[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vamos detalhar o que estamos vendo aqui:\n",
    "\n",
    "- R, G, B e Y são locais de coleta ou entrega.\n",
    "- A letra AZUL indica onde precisamos buscar alguém.\n",
    "- A letra MAGENTA indica para onde aquele passageiro deseja ir.\n",
    "- As linhas sólidas representam paredes que o táxi não pode atravessar.\n",
    "- O retângulo preenchido representa o próprio táxi - é amarelo quando está vazio e verde quando transporta um passageiro."
   ],
   "id": "c96f6c58a51ac946"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nosso mundinho aqui, que chamamos de “ruas”, é uma grade 5x5. O estado deste mundo a qualquer momento pode ser definido por:\n",
    "\n",
    "- Onde está o táxi (um de 5x5 = 25 locais)\n",
    "- Qual é o destino atual (4 possibilidades)\n",
    "- Onde o passageiro está (5 possibilidades: em um dos destinos, ou dentro do táxi)\n",
    "\n",
    "Portanto, há um total de 25 x 4 x 5 = 500 estados possíveis que descrevem o nosso mundo.\n",
    "\n",
    "Para cada estado, existem seis ações possíveis:\n",
    "\n",
    "- Mova-se para o sul, leste, norte ou oeste\n",
    "- Pegue um passageiro\n",
    "- Deixar um passageiro\n",
    "\n",
    "O Q-Learning ocorrerá usando as seguintes recompensas e penalidades em cada estado:\n",
    "\n",
    "- Uma entrega bem-sucedida rende +20 pontos\n",
    "- Cada passo dado enquanto dirige um passageiro gera uma penalidade de -1 ponto\n",
    "- Pegar ou deixar em um local ilegal gera uma penalidade de -10 pontos\n",
    "\n",
    "Atravessar uma parede simplesmente não é permitido.\n",
    "\n",
    "Vamos definir um estado inicial, com o táxi no local (2, 3), o passageiro no local de partida 2 e o destino no local 0:"
   ],
   "id": "9d55eae735f08b33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:25.772595Z",
     "start_time": "2024-09-23T19:40:25.768891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0)\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "streets.render()"
   ],
   "id": "efb6ba8694415234",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001B[43m \u001B[0m: |\n",
      "| | : | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos examinar a tabela de recompensas para este estado inicial:",
   "id": "eb6de157fb0604e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:25.754368Z",
     "start_time": "2024-09-23T19:40:25.749064Z"
    }
   },
   "cell_type": "code",
   "source": "streets.P[initial_state]",
   "id": "1ef943c27249770f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veja como interpretar isso - cada linha corresponde a uma ação potencial neste estado: mover-se para Sul, Norte, Leste ou Oeste, embarque ou desembarque. Os quatro valores em cada linha são a probabilidade atribuída a essa ação, o próximo estado resultante dessa ação, a recompensa por essa ação e se essa ação indica que ocorreu uma desistência bem-sucedida. \n",
    "\n",
    "Assim, por exemplo, mudar para o norte deste estado nos colocaria no estado número 368, incorreria em uma penalidade de -1 por ocupar tempo e não resultaria em uma desistência bem-sucedida.\n",
    "\n",
    "Então, vamos fazer Q-learning! Primeiro precisamos treinar nosso modelo. Em alto nível, treinaremos mais de 10.000 corridas simuladas de táxi. Para cada execução, avançaremos no tempo, com 10% de chance em cada etapa de realizar uma etapa exploratória aleatória, em vez de usar os valores Q aprendidos para guiar nossas ações."
   ],
   "id": "127ff39d372e4591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:30.234066Z",
     "start_time": "2024-09-23T19:40:25.773606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np  # Importa a biblioteca NumPy para operações numéricas\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.1  # Taxa de aprendizado\n",
    "discount_factor = 0.6  # Fator de desconto\n",
    "exploration = 0.1  # Probabilidade de explorar ações aleatórias\n",
    "epochs = 10000  # Número de episódios\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    \n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "            \n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "        \n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "        \n",
    "        state = next_state  # Move para o próximo estado\n"
   ],
   "id": "1635b065e93b4a86",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Portanto, agora temos uma tabela de valores Q que pode ser usada rapidamente para determinar o próximo passo ideal para qualquer estado! Vamos verificar na tabela nosso estado inicial acima:",
   "id": "6acbcc961ac806ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:40:30.242801Z",
     "start_time": "2024-09-23T19:40:30.236075Z"
    }
   },
   "cell_type": "code",
   "source": "q_table[initial_state]",
   "id": "113b5d0b7d7106a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40508849, -2.41992345, -2.40784609, -2.3639511 , -9.60047121,\n",
       "       -8.10058085])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "O valor q mais baixo aqui corresponde à ação \"ir para oeste\", o que faz sentido - essa é a rota mais direta em direção ao nosso destino a partir desse ponto. Parece funcionar! Vamos ver isso em ação!",
   "id": "d32f5ffefa87cfba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import clear_output  # Importa função para limpar a saída do notebook\n",
    "from time import sleep  # Importa função para criar pausas no código\n",
    "\n",
    "# Loop para simular 10 viagens\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    \n",
    "    done = False  # Sinaliza se a viagem terminou\n",
    "    trip_length = 0  # Contador para o comprimento da viagem\n",
    "    \n",
    "    # Loop para executar os passos da viagem, com limite de 25 passos\n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])  # Escolhe a melhor ação com base na Q-table\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "        \n",
    "        clear_output(wait=True)  # Limpa a saída anterior do notebook\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))  # Exibe o número da viagem e o passo atual\n",
    "        print(streets.render(mode='ansi'))  # Renderiza o estado do ambiente em formato ASCII\n",
    "        sleep(0.5)  # Pausa de 0,5 segundos para melhor visualização\n",
    "        state = next_state  # Atualiza o estado atual para o próximo estado\n",
    "        trip_length += 1  # Incrementa o contador do comprimento da viagem\n",
    "        \n",
    "    sleep(2)  # Pausa de 2 segundos antes de iniciar a próxima viagem\n"
   ],
   "id": "cdb29f4d529f228e",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table (supondo que já foi treinada)\n",
    "# Você deve ter a Q-table `q_table` já definida a partir do treinamento anterior\n",
    "\n",
    "# Visualização das viagens do agente\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()  # Reinicia o ambiente para uma nova viagem\n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:  # Limita a 25 passos por viagem\n",
    "        action = np.argmax(q_table[state])  # Escolhe a melhor ação com base na Q-table\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação\n",
    "        \n",
    "        clear_output(wait=True)  # Limpa a saída anterior\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))  # Exibe o número da viagem e o passo atual\n",
    "        print(streets.render(mode='ansi'))  # Renderiza o estado do ambiente no modo texto\n",
    "        sleep(0.5)  # Pausa para visualizar o movimento do agente\n",
    "        \n",
    "        state = next_state  # Atualiza o estado para o próximo passo\n",
    "        trip_length += 1  # Incrementa o comprimento da viagem\n",
    "    \n",
    "    sleep(2)  # Pausa entre as viagens\n"
   ],
   "id": "7a7e96fc7f3879eb",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:43:44.783475Z",
     "start_time": "2024-09-23T19:43:18.671074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.5  # Aumentada para permitir aprendizado mais rápido\n",
    "discount_factor = 0.99  # Aumentado para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.1  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 100000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "id": "f728a7b40e7d57c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 100000 episódios: 0.28653\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:43:57.764134Z",
     "start_time": "2024-09-23T19:43:44.784480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Ajustado para ver se acelera o aprendizado\n",
    "discount_factor = 0.95  # Mantido alto para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 50000  # Aumentado para dar mais tempo de aprendizado\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "id": "b4819ff183a701ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 50000 episódios: 1.7125\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:45:48.314695Z",
     "start_time": "2024-09-23T19:43:57.766141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table com zeros\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Ajustado para ver se acelera o aprendizado\n",
    "discount_factor = 0.95  # Mantido alto para priorizar recompensas futuras\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Aumentado para dar mais tempo de aprendizado\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        random_value = random.uniform(0, 1)  # Gera um valor aleatório entre 0 e 1\n",
    "        if (random_value < initial_exploration):\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Usa a ação com o maior valor Q\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a Q-value usando a fórmula do Q-learning\n",
    "        prev_q = q_table[state, action]  # Valor Q anterior\n",
    "        next_max_q = np.max(q_table[next_state])  # Valor Q máximo do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "        q_table[state, action] = new_q  # Atualiza a Q-table\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "id": "398b427752c99cde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 500000 episódios: 4.95608\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:47:25.909564Z",
     "start_time": "2024-09-23T19:45:48.315702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa duas Q-tables com zeros\n",
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Taxa de aprendizado\n",
    "discount_factor = 0.95  # Fator de desconto\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas\n",
    "reward_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    done = False  # Sinaliza se o episódio terminou\n",
    "    total_reward = 0  # Recompensa total para o episódio\n",
    "\n",
    "    while not done:  # Enquanto o episódio não terminar\n",
    "        # Escolha aleatória entre as duas Q-tables\n",
    "        if random.uniform(0, 1) < initial_exploration:\n",
    "            action = streets.action_space.sample()  # Explora uma ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])\n",
    "\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação e obtém o novo estado\n",
    "\n",
    "        # Atualiza a tabela escolhida com a nova Q-value\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]  # Valor Q anterior\n",
    "            next_max_q = np.max(q_table_b[next_state])  # Valor Q máximo do próximo estado na tabela B\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "            q_table_a[state, action] = new_q  # Atualiza a Q-table A\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]  # Valor Q anterior\n",
    "            next_max_q = np.max(q_table_a[next_state])  # Valor Q máximo do próximo estado na tabela A\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)  # Nova Q-value\n",
    "            q_table_b[state, action] = new_q  # Atualiza a Q-table B\n",
    "\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "        state = next_state  # Move para o próximo estado\n",
    "    \n",
    "    # Armazena a recompensa total do episódio\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas ao longo dos episódios\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Plote a média das recompensas ao longo do tempo\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "plt.show()\n"
   ],
   "id": "83b90c697d1f5e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 500000 episódios: 4.675034\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Seu desafio\n",
    "\n",
    "Modifique o bloco acima para acompanhar os intervalos de tempo totais e use isso como uma métrica para avaliar a qualidade do nosso sistema Q-learning. Você pode querer aumentar o número de viagens simuladas e remover as chamadas sleep() para permitir a execução de mais amostras.\n",
    "\n",
    "Agora, tente fazer experiências com os hiperparâmetros. Quão baixo pode chegar o número de épocas antes que nosso modelo comece a sofrer? Você consegue encontrar melhores taxas de aprendizagem, fatores de desconto ou fatores de exploração para tornar o treinamento mais eficiente? A taxa de exploração versus exploração, em particular, é interessante de experimentar."
   ],
   "id": "e63ee34a501e05fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:50:41.627321Z",
     "start_time": "2024-09-23T19:50:41.619890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym  # Importa a biblioteca gym para ambientes de aprendizado por reforço\n",
    "import random  # Importa a biblioteca random para geração de números aleatórios\n",
    "import numpy as np  # Importa a biblioteca numpy para operações numéricas\n",
    "import time  # Importa a biblioteca time para medir intervalos de tempo\n",
    "\n",
    "# Define a semente para a geração de números aleatórios, garantindo reprodutibilidade\n",
    "random.seed(1234)\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Renderiza o ambiente, exibindo a interface gráfica\n",
    "streets.render()\n"
   ],
   "id": "eff815c969c398a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :\u001B[34;1mG\u001B[0m|\n",
      "| : | : : |\n",
      "| : :\u001B[43m \u001B[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:52:29.662925Z",
     "start_time": "2024-09-23T19:50:42.951129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Configurações de aprendizado\n",
    "learning_rate = 0.4  # Taxa de aprendizado\n",
    "discount_factor = 0.95  # Fator de desconto\n",
    "initial_exploration = 1.0  # Taxa de exploração inicial\n",
    "final_exploration = 0.05  # Taxa de exploração final\n",
    "decay_rate = 0.995  # Taxa de decaimento da exploração\n",
    "epochs = 500000  # Número de episódios\n",
    "\n",
    "# Lista para armazenar as recompensas e tempos de viagem\n",
    "reward_list = []\n",
    "time_list = []\n",
    "\n",
    "# Loop principal de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    \n",
    "    done = False  # Sinaliza se a viagem terminou\n",
    "    trip_length = 0  # Contador para o comprimento da viagem\n",
    "    start_time = time.time()  # Marca o tempo inicial da viagem\n",
    "    \n",
    "    while not done and trip_length < 25:  # Limita a 25 passos\n",
    "        action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])  # Escolhe a ação\n",
    "        next_state, reward, done, info = streets.step(action)  # Executa a ação\n",
    "        \n",
    "        # Atualiza a tabela escolhida com a nova Q-value\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]\n",
    "            next_max_q = np.max(q_table_b[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_a[state, action] = new_q\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]\n",
    "            next_max_q = np.max(q_table_a[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_b[state, action] = new_q\n",
    "\n",
    "        state = next_state  # Atualiza o estado atual\n",
    "        trip_length += 1  # Incrementa o contador do comprimento da viagem\n",
    "    \n",
    "    total_time = time.time() - start_time  # Calcula o tempo total da viagem\n",
    "    reward_list.append(reward)  # Armazena a recompensa do episódio\n",
    "    time_list.append(total_time)  # Armazena o tempo da viagem\n",
    "\n",
    "    # Reduz a taxa de exploração\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Exibe a média das recompensas e o tempo médio das viagens\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "print(f\"Tempo médio das viagens: {np.mean(time_list)} segundos\")\n",
    "\n",
    "# Plote as recompensas e tempos ao longo do tempo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.convolve(time_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 viagens)')\n",
    "plt.ylabel('Tempo médio (segundos)')\n",
    "plt.title('Tempo Médio das Viagens em Taxi-v3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c3e8c0146aa2bf79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 500000 episódios: 19.883954\n",
      "Tempo médio das viagens: 0.00016682219982147218 segundos\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:55:18.012210Z",
     "start_time": "2024-09-23T19:53:37.088833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurações iniciais\n",
    "random.seed(1234)\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa as Q-tables\n",
    "q_table_a = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "q_table_b = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.2  # Taxa de aprendizado ajustada\n",
    "discount_factor = 0.99  # Fator de desconto ajustado\n",
    "initial_exploration = 1.0\n",
    "final_exploration = 0.05\n",
    "decay_rate = 0.995\n",
    "epochs = 500000\n",
    "\n",
    "# Armazenamento de recompensas e tempos\n",
    "reward_list = []\n",
    "time_list = []\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table_a[state]) if random.random() < 0.5 else np.argmax(q_table_b[state])\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        # Atualiza as Q-tables\n",
    "        if random.random() < 0.5:\n",
    "            prev_q = q_table_a[state, action]\n",
    "            next_max_q = np.max(q_table_b[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_a[state, action] = new_q\n",
    "        else:\n",
    "            prev_q = q_table_b[state, action]\n",
    "            next_max_q = np.max(q_table_a[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table_b[state, action] = new_q\n",
    "\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    reward_list.append(reward)\n",
    "    time_list.append(total_time)\n",
    "\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "print(f\"Tempo médio das viagens: {np.mean(time_list)} segundos\")\n",
    "\n",
    "# Gráficos\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.convolve(time_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 viagens)')\n",
    "plt.ylabel('Tempo médio (segundos)')\n",
    "plt.title('Tempo Médio das Viagens em Taxi-v3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a47443bf02bdf589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 500000 episódios: 19.818566\n",
      "Tempo médio das viagens: 0.00017559904861450196 segundos\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:56:46.467545Z",
     "start_time": "2024-09-23T19:55:18.028384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SARSA\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cria o ambiente \"Taxi-v3\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Inicializa a Q-table\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.2\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "final_exploration = 0.05\n",
    "decay_rate = 0.995\n",
    "epochs = 500000\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    action = streets.action_space.sample()  # Ação inicial aleatória\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        # Atualiza a Q-value usando a regra SARSA\n",
    "        next_action = streets.action_space.sample() if random.uniform(0, 1) < initial_exploration else np.argmax(q_table[next_state])\n",
    "        q_table[state, action] += learning_rate * (reward + discount_factor * q_table[next_state, next_action] - q_table[state, action])\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "    reward_list.append(reward)\n",
    "    initial_exploration = max(final_exploration, initial_exploration * decay_rate)\n",
    "\n",
    "print(f\"Média das recompensas após {epochs} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Gráficos\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.title('Desempenho do Agente em Taxi-v3 com SARSA')\n",
    "plt.show()\n"
   ],
   "id": "160454faed0ae242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das recompensas após 500000 episódios: 20.0\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-23T20:00:38.866961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym  # Importa a biblioteca gym para ambientes de aprendizado por reforço\n",
    "import random  # Importa a biblioteca random para geração de números aleatórios\n",
    "import numpy as np  # Importa o NumPy para manipulação de arrays\n",
    "import torch  # Importa o PyTorch para a construção da rede neural\n",
    "import torch.nn as nn  # Importa o módulo de redes neurais do PyTorch\n",
    "import torch.optim as optim  # Importa o módulo de otimização do PyTorch\n",
    "from collections import deque  # Importa deque para armazenar a memória de experiências\n",
    "import matplotlib.pyplot as plt  # Importa matplotlib para visualização\n",
    "\n",
    "# Definindo a rede neural para o DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # Camada oculta 1: recebe o estado e tem 24 neurônios\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        # Camada oculta 2: recebe a saída da primeira camada e também tem 24 neurônios\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        # Camada de saída: retorna os valores Q para cada ação possível\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passagem de entrada pela primeira camada com função de ativação ReLU\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Passagem pela segunda camada com função de ativação ReLU\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # Retorna a saída da camada de saída\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hiperparâmetros\n",
    "EPISODES = 5000  # Número total de episódios de treinamento\n",
    "LEARNING_RATE = 0.001  # Taxa de aprendizado para atualização da rede neural\n",
    "GAMMA = 0.95  # Fator de desconto para considerar recompensas futuras\n",
    "EXPLORATION_MAX = 1.0  # Taxa máxima de exploração (percentual de ações aleatórias)\n",
    "EXPLORATION_MIN = 0.01  # Taxa mínima de exploração\n",
    "EXPLORATION_DECAY = 0.995  # Fator de decaimento da taxa de exploração\n",
    "MEMORY_SIZE = 2000  # Tamanho máximo da memória de experiências\n",
    "BATCH_SIZE = 32  # Tamanho do mini-lote para atualização da rede\n",
    "\n",
    "# Inicializando o ambiente e os parâmetros\n",
    "env = gym.make(\"Taxi-v3\")  # Cria o ambiente Taxi-v3\n",
    "state_size = env.observation_space.n  # Obtém o número total de estados\n",
    "action_size = env.action_space.n  # Obtém o número total de ações\n",
    "dqn = DQN(state_size, action_size)  # Inicializa a rede DQN\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=LEARNING_RATE)  # Otimizador Adam\n",
    "loss_fn = nn.MSELoss()  # Função de perda para minimizar a diferença dos valores Q\n",
    "\n",
    "# Memória para armazenar experiências\n",
    "memory = deque(maxlen=MEMORY_SIZE)  # Inicializa a memória de experiências\n",
    "reward_list = []  # Lista para armazenar recompensas totais de cada episódio\n",
    "\n",
    "# Treinamento\n",
    "for episode in range(EPISODES):  # Para cada episódio\n",
    "    state = env.reset()  # Reinicializa o ambiente e obtém o estado inicial\n",
    "    state = np.identity(state_size)[state]  # Codificação one-hot do estado\n",
    "    total_reward = 0  # Inicializa a recompensa total para este episódio\n",
    "    exploration_rate = EXPLORATION_MAX  # Taxa de exploração inicial\n",
    "\n",
    "    for time in range(50):  # Limite de passos por episódio\n",
    "        # Escolhendo uma ação com a política ε-greedy\n",
    "        if random.random() <= exploration_rate:\n",
    "            action = env.action_space.sample()  # Ação aleatória\n",
    "        else:\n",
    "            with torch.no_grad():  # Desliga o cálculo de gradientes\n",
    "                action = torch.argmax(dqn(torch.FloatTensor(state))).item()  # Ação com maior valor Q\n",
    "\n",
    "        # Executa a ação escolhida no ambiente\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.identity(state_size)[next_state]  # Codificação one-hot do próximo estado\n",
    "        memory.append((state, action, reward, next_state, done))  # Armazena a experiência\n",
    "\n",
    "        state = next_state  # Atualiza o estado atual\n",
    "        total_reward += reward  # Acumula a recompensa total\n",
    "\n",
    "        if done:  # Se a viagem foi concluída\n",
    "            break  # Sai do loop\n",
    "\n",
    "    # Experiência de amostragem e treinamento\n",
    "    if len(memory) > BATCH_SIZE:  # Se a memória tiver experiências suficientes\n",
    "        mini_batch = random.sample(memory, BATCH_SIZE)  # Amostra aleatória de experiências\n",
    "        for m_state, m_action, m_reward, m_next_state, m_done in mini_batch:\n",
    "            target = m_reward  # Começa com a recompensa recebida\n",
    "            if not m_done:  # Se o episódio não terminou\n",
    "                # Adiciona a recompensa descontada do próximo estado\n",
    "                target += GAMMA * torch.max(dqn(torch.FloatTensor(m_next_state))).item()\n",
    "            # Calcula o valor Q para a ação escolhida\n",
    "            target_f = dqn(torch.FloatTensor(m_state))\n",
    "            target_f[m_action] = target  # Atualiza o valor Q da ação escolhida\n",
    "            \n",
    "            # Otimização\n",
    "            optimizer.zero_grad()  # Zera os gradientes acumulados\n",
    "            loss = loss_fn(dqn(torch.FloatTensor(m_state)), target_f)  # Calcula a perda\n",
    "            loss.backward()  # Calcula o gradiente da perda\n",
    "            optimizer.step()  # Atualiza os pesos da rede\n",
    "\n",
    "    # Atualiza a taxa de exploração\n",
    "    if exploration_rate > EXPLORATION_MIN:\n",
    "        exploration_rate *= EXPLORATION_DECAY  # Decai a taxa de exploração\n",
    "\n",
    "    reward_list.append(total_reward)  # Adiciona a recompensa total da rodada\n",
    "\n",
    "# Resultados\n",
    "print(f\"Média das recompensas após {EPISODES} episódios: {np.mean(reward_list)}\")\n",
    "\n",
    "# Gráficos\n",
    "plt.plot(np.convolve(reward_list, np.ones(100)/100, mode='valid'))  # Plota a média das recompensas\n",
    "plt.xlabel('Episódios (média das últimas 100 recompensas)')  # Rótulo do eixo X\n",
    "plt.ylabel('Recompensa média')  # Rótulo do eixo Y\n",
    "plt.title('Desempenho do Agente em Taxi-v3 com DQN')  # Título do gráfico\n",
    "plt.show()  # Mostra o gráfico\n",
    "\n",
    "# Fechar o ambiente\n",
    "env.close()  # Fecha o ambiente para liberar recursos\n"
   ],
   "id": "404fd210f60d2238",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "34fbabd27e64e81d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
