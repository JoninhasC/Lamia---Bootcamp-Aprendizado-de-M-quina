{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering and the Curse of Dimensionality\n",
    "\n",
    "Engenharia de Características é um processo crucial em aprendizado de máquina que envolve a seleção, transformação e criação de atributos relevantes a partir dos dados de treinamento. A qualidade das características utilizadas para treinar um modelo pode influenciar significativamente sua precisão e eficácia.\n",
    "\n",
    "Principais Componentes:\n",
    "Seleção de Características: Identificação de quais atributos são relevantes e eliminação de características irrelevantes.\n",
    "Transformação de Características: Modificação de dados brutos (como normalização, escalonamento ou codificação) para torná-los adequados ao modelo.\n",
    "Tratamento de Dados Ausentes: Estratégias para lidar com lacunas nos dados, que são comuns em cenários do mundo real.\n",
    "Criação de Novas Características: Combinação ou transformação de características existentes para melhorar a representação dos dados.\n",
    "Maldição da Dimensionalidade:\n",
    "O aumento do número de características pode tornar o espaço de solução mais complexo e difícil de navegar, resultando em dados esparsos. Portanto, é importante reduzir a dimensionalidade para facilitar a busca por soluções ideais.\n",
    "\n",
    "Técnicas:\n",
    "Análise de Componentes Principais (PCA): Reduz a dimensionalidade enquanto preserva a maior quantidade de informação possível.\n",
    "Agrupamento K-Means: Uma técnica não supervisionada que condensa características em um conjunto menor baseado em similaridades."
   ],
   "id": "3bb8d6158aeb60b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T21:55:10.478274Z",
     "start_time": "2024-09-24T21:55:09.012933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Criação de um conjunto de dados fictício\n",
    "data = {\n",
    "    'idade': [25, 32, 47, 51, 23, 36, 45, 29],\n",
    "    'altura': [1.75, 1.80, 1.60, 1.70, 1.65, 1.85, 1.78, 1.72],\n",
    "    'peso': [70, 80, 65, 75, 55, 90, 85, 78],\n",
    "    'salario': [30000, 45000, 60000, 80000, 25000, 70000, 75000, 50000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Exibir dados originais\n",
    "print(\"Dados Originais:\")\n",
    "print(df)\n",
    "\n",
    "# Seleção de características (remover 'salario' como exemplo)\n",
    "X = df.drop(columns=['salario'])\n",
    "y = df['salario']\n",
    "\n",
    "# Tratamento de dados ausentes (exemplo fictício, não temos dados ausentes aqui)\n",
    "# X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Transformação: Normalização\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Redução de Dimensionalidade: PCA\n",
    "pca = PCA(n_components=2)  # Reduzir para 2 dimensões\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Divisão do conjunto de dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nDados após Engenharia de Características:\")\n",
    "print(\"X_train:\", X_train)\n",
    "print(\"y_train:\", y_train.values)\n"
   ],
   "id": "17962f9db5c8d0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados Originais:\n",
      "   idade  altura  peso  salario\n",
      "0     25    1.75    70    30000\n",
      "1     32    1.80    80    45000\n",
      "2     47    1.60    65    60000\n",
      "3     51    1.70    75    80000\n",
      "4     23    1.65    55    25000\n",
      "5     36    1.85    90    70000\n",
      "6     45    1.78    85    75000\n",
      "7     29    1.72    78    50000\n",
      "\n",
      "Dados após Engenharia de Características:\n",
      "X_train: [[-0.32077247 -1.17687822]\n",
      " [ 0.01767663 -0.5793409 ]\n",
      " [-1.67395832  1.49681774]\n",
      " [-2.26785679 -1.10581517]\n",
      " [-0.03883413  1.55948711]\n",
      " [ 1.26920949  0.75939886]]\n",
      "y_train: [30000 50000 60000 25000 80000 75000]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explicação do Código\n",
    "Criação de um Conjunto de Dados: Um DataFrame é criado com características fictícias como idade, altura, peso e salário.\n",
    "Seleção de Características: A coluna 'salario' é removida para focar nas características preditivas.\n",
    "Normalização: As características são normalizadas usando StandardScaler para que todas tenham a mesma escala.\n",
    "Redução de Dimensionalidade: O PCA é aplicado para reduzir as características a 2 dimensões.\n",
    "Divisão dos Dados: Os dados são divididos em conjuntos de treinamento e teste."
   ],
   "id": "aa1fb124cd0c70fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imputation Techniques for Missing Data\n",
    "\n",
    "\n",
    "Resumo sobre Imputação de Dados Ausentes\n",
    "A imputação de dados ausentes é uma etapa crucial na engenharia de características, especialmente em cenários do mundo real, onde a falta de dados é comum. Este processo envolve preencher os valores ausentes em um conjunto de dados para que análises e modelos de aprendizado de máquina possam ser aplicados efetivamente.\n",
    "\n",
    "Estratégias Comuns para Imputação\n",
    "Substituição pela Média:\n",
    "\n",
    "Consiste em substituir os valores ausentes pela média da coluna correspondente. É rápida e fácil de implementar, mas tem desvantagens:\n",
    "Não leva em consideração as correlações entre características.\n",
    "Pode ser afetada por outliers (valores atípicos).\n",
    "Não é aplicável a características categóricas.\n",
    "Substituição pela Mediana:\n",
    "\n",
    "Uma alternativa à média, mais robusta a outliers. Em conjuntos de dados com valores extremos, a mediana pode fornecer uma imputação mais representativa.\n",
    "K-Nearest Neighbors (KNN):\n",
    "\n",
    "Essa técnica usa a média dos valores dos K vizinhos mais próximos para imputar dados ausentes. É mais eficaz para dados numéricos e considera as relações entre características.\n",
    "Regressão Múltipla:\n",
    "\n",
    "Modelos de regressão podem ser usados para prever os valores ausentes com base nas características disponíveis.\n",
    "MICE (Imputação Múltipla por Equações Encadeadas):\n",
    "\n",
    "Uma abordagem avançada que trata a imputação como um processo de múltiplas fases, levando em conta as incertezas e interações entre variáveis.\n",
    "Coleta de Mais Dados:\n",
    "\n",
    "A melhor solução para dados ausentes é, muitas vezes, coletar mais dados de qualidade. Isso ajuda a minimizar as lacunas e melhora a precisão do modelo.\n",
    "Implementação da Imputação de Dados Ausentes\n",
    "Abaixo, segue um exemplo em Python utilizando a biblioteca pandas para demonstrar a imputação de dados ausentes usando a substituição pela média e a técnica KNN."
   ],
   "id": "d1997f280191f9d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T21:56:48.914516Z",
     "start_time": "2024-09-24T21:56:46.329611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Criação de um conjunto de dados fictício com dados ausentes\n",
    "data = {\n",
    "    'idade': [25, 32, None, 51, 23, 36, None, 29],\n",
    "    'altura': [1.75, 1.80, 1.60, None, 1.65, 1.85, 1.78, 1.72],\n",
    "    'peso': [70, None, 65, 75, 55, 90, 85, 78]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dados Originais:\")\n",
    "print(df)\n",
    "\n",
    "# Imputação pela média\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df_mean_imputed = pd.DataFrame(mean_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDados após Imputação pela Média:\")\n",
    "print(df_mean_imputed)\n",
    "\n",
    "# Imputação KNN\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDados após Imputação KNN:\")\n",
    "print(df_knn_imputed)\n"
   ],
   "id": "98ee2a1963ef1688",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados Originais:\n",
      "   idade  altura  peso\n",
      "0   25.0    1.75  70.0\n",
      "1   32.0    1.80   NaN\n",
      "2    NaN    1.60  65.0\n",
      "3   51.0     NaN  75.0\n",
      "4   23.0    1.65  55.0\n",
      "5   36.0    1.85  90.0\n",
      "6    NaN    1.78  85.0\n",
      "7   29.0    1.72  78.0\n",
      "\n",
      "Dados após Imputação pela Média:\n",
      "       idade    altura  peso\n",
      "0  25.000000  1.750000  70.0\n",
      "1  32.000000  1.800000  74.0\n",
      "2  32.666667  1.600000  65.0\n",
      "3  51.000000  1.735714  75.0\n",
      "4  23.000000  1.650000  55.0\n",
      "5  36.000000  1.850000  90.0\n",
      "6  32.666667  1.780000  85.0\n",
      "7  29.000000  1.720000  78.0\n",
      "\n",
      "Dados após Imputação KNN:\n",
      "       idade    altura  peso\n",
      "0  25.000000  1.750000  70.0\n",
      "1  32.000000  1.800000  76.0\n",
      "2  26.666667  1.600000  65.0\n",
      "3  51.000000  1.743333  75.0\n",
      "4  23.000000  1.650000  55.0\n",
      "5  36.000000  1.850000  90.0\n",
      "6  32.333333  1.780000  85.0\n",
      "7  29.000000  1.720000  78.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handling Unbalanced Data: Oversampling, Undersampling, and SMOTE \n",
    "Dados Desequilibrados\n",
    "Definição: Dados desequilibrados ocorrem quando há uma grande discrepância entre as classes em um conjunto de dados, como casos positivos (fraudes) e negativos (não fraudes).\n",
    "Exemplo: Na detecção de fraudes, a fraudes representam uma pequena fração do total (ex: 0,01%), levando o modelo a prever predominantemente a classe negativa, resultando em alta precisão, mas baixa capacidade de detectar fraudes.\n",
    "Desafios\n",
    "Modelos treinados em conjuntos de dados desequilibrados podem aprender a simplesmente prever a classe majoritária, ignorando a classe minoritária.\n",
    "Isso pode resultar em um desempenho ruim na detecção de fraudes, onde a identificação correta é crítica.\n",
    "Soluções para Dados Desequilibrados\n",
    "Sobreamostragem:\n",
    "\n",
    "Descrição: Consiste em aumentar o número de amostras da classe minoritária (ex: fraudes) replicando-as.\n",
    "Vantagem: Permite que o modelo tenha mais dados para aprender a identificar fraudes.\n",
    "Desvantagem: Pode levar a overfitting, pois o modelo aprende padrões apenas de dados duplicados.\n",
    "Subamostragem:\n",
    "\n",
    "Descrição: Reduzir o número de amostras da classe majoritária (ex: não fraudes).\n",
    "Vantagem: Equilibra as classes.\n",
    "Desvantagem: Pode resultar na perda de informações importantes, comprometendo a precisão do modelo.\n",
    "SMOTE (Synthetic Minority Oversampling Technique):\n",
    "\n",
    "Descrição: Gera novas amostras da classe minoritária utilizando técnicas de interpolação entre os vizinhos mais próximos.\n",
    "Vantagem: Cria dados sintéticos que são representativos e diversificados, melhorando a capacidade do modelo de aprender sobre a classe minoritária.\n",
    "Desvantagem: Requer mais processamento e pode aumentar o tempo de treinamento.\n",
    "Ajuste de Limites de Decisão:\n",
    "\n",
    "Descrição: Modificar o limiar de probabilidade que determina se um caso é classificado como positivo (ex: fraude) ou negativo.\n",
    "Vantagem: Permite controlar a taxa de falsos positivos e negativos de acordo com as prioridades do negócio (ex: minimizar alarmes falsos).\n",
    "Desvantagem: Necessita de uma análise cuidadosa sobre o impacto das alterações no limite, já que pode afetar a detecção de fraudes.\n",
    "Considerações Finais\n",
    "Análise de Custos: É importante avaliar o custo de falsos positivos (alarmar clientes desnecessariamente) versus falsos negativos (não detectar fraudes reais). A estratégia pode variar dependendo do contexto e do impacto sobre os clientes.\n",
    "Abordagem Híbrida: Combinar diferentes métodos pode levar a melhores resultados, balanceando o aumento da classe minoritária com técnicas de ajuste de limites.\n",
    "Importância do Contexto: Cada abordagem deve ser escolhida com base na natureza dos dados, nos objetivos do modelo e nas consequências das decisões erradas."
   ],
   "id": "4b444e5f0d0ffc2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Binning, Transforming, Encoding, Scaling, and Shuffling\n",
    "1. Fiação (Binning)\n",
    "Definição: O processo de transformar dados numéricos em dados categóricos, agrupando valores em faixas (bins).\n",
    "Exemplo: Agrupar idades em intervalos, como 20-29, 30-39, etc.\n",
    "Objetivo: Reduzir a complexidade dos dados e lidar com incertezas nas medições. Facilita o uso de modelos que operam melhor com dados categóricos.\n",
    "Considerações: Pode resultar em perda de informação, então deve ser usado com cautela, especialmente se os dados originais forem precisos.\n",
    "2. Quantile Binning\n",
    "Definição: Uma forma de fiação que distribui as amostras uniformemente entre os bins.\n",
    "Vantagem: Garante que cada bin tenha o mesmo número de amostras, o que pode ser útil para evitar viés na modelagem.\n",
    "3. Transformação de Dados\n",
    "Objetivo: Aplicar funções (ex: logaritmos, potências) para tornar os dados mais adequados para algoritmos de aprendizado de máquina.\n",
    "Exemplo: Transformar dados exponenciais usando uma transformação logarítmica para linearizar a relação.\n",
    "Uso: Pode melhorar a capacidade do modelo de capturar padrões não lineares, ajudando a revelar tendências ocultas nos dados.\n",
    "4. Codificação (Encoding)\n",
    "Codificação One-Hot: Transforma categorias em colunas binárias, onde cada coluna representa uma categoria específica (0 ou 1).\n",
    "Exemplo: Para reconhecimento de dígitos (0 a 9), cada número é representado por 10 colunas.\n",
    "Vantagem: Essencial em redes neurais, onde os dados precisam ser representados de forma binária para ativação dos neurônios.\n",
    "5. Escalonamento e Normalização\n",
    "Descrição: Processos para garantir que as características estejam em escalas comparáveis.\n",
    "Importância: Modelos sensíveis à escala (como SVM, KNN) podem ser afetados se as variáveis têm magnitudes muito diferentes.\n",
    "Métodos:\n",
    "Min-Max Scaling: Reduz os dados para uma faixa específica (ex: 0 a 1).\n",
    "Standardization: Ajusta os dados para que tenham média 0 e desvio padrão 1.\n",
    "6. Embaralhamento (Shuffling)\n",
    "Objetivo: Aleatorizar a ordem dos dados de treinamento para eliminar padrões que possam introduzir viés.\n",
    "Importância: A ordem de coleta dos dados pode afetar os resultados do modelo; embaralhar pode melhorar a qualidade do treinamento.\n"
   ],
   "id": "c9207c82143538e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c93dfb25f0766f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
