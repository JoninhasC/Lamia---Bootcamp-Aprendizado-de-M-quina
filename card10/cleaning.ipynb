{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cleaning Your Data\n",
    "\n",
    "Vamos pegar um registro de acesso à web e descobrir a partir dele as páginas mais visualizadas de um site! Parece fácil, certo?\n",
    "\n",
    "Vamos configurar uma regex que nos permita analisar uma linha de log de acesso do Apache:"
   ],
   "id": "15df33723086784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:09:44.025903Z",
     "start_time": "2024-09-24T19:09:44.022550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re  # Importa a biblioteca de expressões regulares\n",
    "\n",
    "# Compila uma expressão regular para capturar dados de um log de acesso de servidor web\n",
    "format_pat = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"             # Captura o endereço IP do cliente (ex: 192.168.1.1)\n",
    "    r\"(?P<identity>\\S*)\\s\"             # Captura a identidade do cliente (geralmente um hífen)\n",
    "    r\"(?P<user>\\S*)\\s\"                 # Captura o nome de usuário se autenticado (geralmente um hífen)\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"              # Captura a hora do acesso, entre colchetes (ex: [01/Jan/2024:00:00:01 +0000])\n",
    "    r'\"(?P<request>.*?)\"\\s'             # Captura a solicitação HTTP (ex: \"GET / HTTP/1.1\")\n",
    "    r\"(?P<status>\\d+)\\s\"                # Captura o código de status HTTP (ex: 200)\n",
    "    r\"(?P<bytes>\\S*)\\s\"                 # Captura o número de bytes enviados (geralmente um hífen se não houver dados)\n",
    "    r'\"(?P<referer>.*?)\"\\s'             # Captura a URL referida (de onde o usuário veio, pode ser um hífen)\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'         # Captura a informação do agente do usuário (navegador, sistema operacional, etc.)\n",
    ")\n"
   ],
   "id": "e1ed27646362facb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aqui está o caminho para o arquivo de log que estou analisando:",
   "id": "9901f4bdf7b3fd1c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-24T19:09:44.029501Z",
     "start_time": "2024-09-24T19:09:44.026909Z"
    }
   },
   "source": "logPath = \"access_log.txt\"",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agora vamos preparar um pequeno script para extrair a URL em cada acesso e usar um dicionário para contar o número de vezes que cada uma aparece. Em seguida, classificaremos e imprimiremos as 20 páginas principais. O que poderia dar errado?",
   "id": "9ae1f9970846253e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:09:44.098408Z",
     "start_time": "2024-09-24T19:09:44.030505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dicionário para armazenar a contagem de acessos por URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log no modo de leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer o match da linha com a expressão regular definida anteriormente\n",
    "        match = format_pat.match(line)\n",
    "        if match:  # Se houver uma correspondência\n",
    "            access = match.groupdict()  # Obtém os dados correspondentes como um dicionário\n",
    "            request = access['request']  # Extrai a solicitação HTTP\n",
    "            \n",
    "            # Divide a solicitação nos seus componentes (ação, URL e protocolo)\n",
    "            (action, URL, protocol) = request.split()\n",
    "            \n",
    "            # Verifica se a URL já está no dicionário\n",
    "            if URL in URLCounts:\n",
    "                URLCounts[URL] = URLCounts[URL] + 1  # Incrementa a contagem da URL\n",
    "            else:\n",
    "                URLCounts[URL] = 1  # Inicializa a contagem da URL\n",
    "\n",
    "# Classifica as URLs com base em suas contagens, do maior para o menor\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas e suas contagens\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n"
   ],
   "id": "74146acf05aeb5b1",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m request \u001B[38;5;241m=\u001B[39m access[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# Extrai a solicitação HTTP\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Divide a solicitação em seus componentes (ação, URL e protocolo)\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m (action, URL, protocol) \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39msplit()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Verifica se a URL já está no dicionário\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m URL \u001B[38;5;129;01min\u001B[39;00m URLCounts:\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hum. A parte 'solicitação' da linha deve ser parecida com isto:\n",
    "\n",
    "OBTER /blog/HTTP/1.1\n",
    "\n",
    "Deve haver uma ação HTTP, a URL e o protocolo. Mas parece que isso nem sempre acontece. Vamos imprimir solicitações que não contenham três itens:"
   ],
   "id": "f90bca2dfc63ab57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:09:54.367588Z",
     "start_time": "2024-09-24T19:09:54.152397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dicionário para armazenar a contagem de acessos por URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log no modo de leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer o match da linha com a expressão regular definida anteriormente\n",
    "        match = format_pat.match(line)\n",
    "        if match:  # Se houver uma correspondência\n",
    "            access = match.groupdict()  # Obtém os dados correspondentes como um dicionário\n",
    "            request = access['request']  # Extrai a solicitação HTTP\n",
    "            \n",
    "            # Divide a solicitação em seus componentes\n",
    "            fields = request.split()\n",
    "            \n",
    "            # Verifica se o número de campos extraídos é diferente de 3\n",
    "            if (len(fields) != 3):\n",
    "                print(fields)  # Imprime os campos se não corresponderem ao esperado\n"
   ],
   "id": "bb5b6b72bc1a5989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_\\\\xb0ZP\\\\x07tR\\\\xe5']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Huh. Além dos campos vazios, existe um que contém apenas lixo. Bem, vamos modificar nosso script para verificar esse caso:",
   "id": "e62fdde4646ba4a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:11:55.233652Z",
     "start_time": "2024-09-24T19:11:55.000015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dicionário para armazenar a contagem de acessos por URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log no modo de leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer o match da linha com a expressão regular definida anteriormente\n",
    "        match = format_pat.match(line)\n",
    "        if match:  # Se houver uma correspondência\n",
    "            access = match.groupdict()  # Obtém os dados correspondentes como um dicionário\n",
    "            request = access['request']  # Extrai a solicitação HTTP\n",
    "            \n",
    "            # Divide a solicitação em seus componentes\n",
    "            fields = request.split()\n",
    "            \n",
    "            # Verifica se o número de campos extraídos é igual a 3\n",
    "            if (len(fields) == 3):\n",
    "                URL = fields[1]  # A URL é o segundo campo da solicitação\n",
    "                \n",
    "                # Atualiza a contagem de acessos para a URL\n",
    "                if URL in URLCounts:\n",
    "                    URLCounts[URL] = URLCounts[URL] + 1  # Incrementa a contagem existente\n",
    "                else:\n",
    "                    URLCounts[URL] = 1  # Inicializa a contagem se a URL ainda não estiver no dicionário\n",
    "\n",
    "# Ordena as URLs por contagem de acessos em ordem decrescente\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas e suas contagens\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n"
   ],
   "id": "446256af5900e25f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xmlrpc.php: 68494\n",
      "/wp-login.php: 1923\n",
      "/: 440\n",
      "/blog/: 138\n",
      "/robots.txt: 123\n",
      "/sitemap_index.xml: 118\n",
      "/post-sitemap.xml: 118\n",
      "/page-sitemap.xml: 117\n",
      "/category-sitemap.xml: 117\n",
      "/orlando-headlines/: 95\n",
      "/san-jose-headlines/: 85\n",
      "http://51.254.206.142/httptest.php: 81\n",
      "/comics-2/: 76\n",
      "/travel/: 74\n",
      "/entertainment/: 72\n",
      "/business/: 70\n",
      "/national/: 70\n",
      "/national-headlines/: 70\n",
      "/world/: 70\n",
      "/weather/: 70\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Funcionou! Mas os resultados realmente não fazem sentido. O que realmente queremos são páginas acessadas por humanos reais em busca de notícias do nosso pequeno site de notícias. O que diabos é xmlrpc.php? Uma olhada no próprio log revela muitas entradas como esta:\n",
    "\n",
    "46.166.139.20 - - [05/Dez/2015:05:19:35 +0000] \"POST /xmlrpc.php HTTP/1.0\" 200 370 \"-\" \"Mozilla/4.0 (compatível: MSIE 7.0; Windows NT 6.0)\"\n",
    "\n",
    "Não tenho muita certeza do que o script faz, mas indica que não estamos apenas processando ações GET. Não queremos POSTS, então vamos filtrá-los:"
   ],
   "id": "9fe72fc07104e99f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:16:33.516213Z",
     "start_time": "2024-09-24T19:16:33.281966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa um dicionário para contar o número de acessos a cada URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer uma correspondência da linha com o padrão definido\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            # Se a linha corresponder ao padrão, armazena os grupos correspondentes em um dicionário\n",
    "            access = match.groupdict()\n",
    "            request = access['request']  # Extrai a parte da solicitação da linha do log\n",
    "            fields = request.split()  # Divide a solicitação em seus componentes (ação, URL, protocolo)\n",
    "\n",
    "            # Verifica se a solicitação tem exatamente 3 campos\n",
    "            if (len(fields) == 3):\n",
    "                (action, URL, protocol) = fields  # Desempacota os campos em variáveis separadas\n",
    "\n",
    "                # Conta apenas as solicitações do tipo GET\n",
    "                if (action == 'GET'):\n",
    "                    # Se a URL já estiver no dicionário, incrementa sua contagem\n",
    "                    if URL in URLCounts:\n",
    "                        URLCounts[URL] = URLCounts[URL] + 1\n",
    "                    else:\n",
    "                        # Caso contrário, inicializa a contagem da URL em 1\n",
    "                        URLCounts[URL] = 1\n",
    "\n",
    "# Ordena as URLs com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n"
   ],
   "id": "9f675d477d60aeaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 434\n",
      "/blog/: 138\n",
      "/robots.txt: 123\n",
      "/sitemap_index.xml: 118\n",
      "/post-sitemap.xml: 118\n",
      "/page-sitemap.xml: 117\n",
      "/category-sitemap.xml: 117\n",
      "/orlando-headlines/: 95\n",
      "/san-jose-headlines/: 85\n",
      "http://51.254.206.142/httptest.php: 81\n",
      "/comics-2/: 76\n",
      "/travel/: 74\n",
      "/entertainment/: 72\n",
      "/business/: 70\n",
      "/national/: 70\n",
      "/national-headlines/: 70\n",
      "/world/: 70\n",
      "/weather/: 70\n",
      "/about/: 69\n",
      "/defense-sticking-head-sand/: 69\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Isso está começando a parecer melhor. Mas este é um site de notícias - as pessoas estão realmente lendo o pequeno blog nele em vez de páginas de notícias? Isso não faz sentido. Vejamos uma entrada /blog/ típica no log:\n",
    "\n",
    "54.165.199.171 - - [05/dez/2015:09:32:05 +0000] \"OBTER /blog/ HTTP/1.0\" 200 31670 \"-\" \"-\"\n",
    "\n",
    "Hum. Por que o agente do usuário está em branco? Parece algum tipo de raspador malicioso ou algo assim. Vamos descobrir com quais agentes de usuário estamos lidando:"
   ],
   "id": "c462ef800bab2757"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:17:38.726438Z",
     "start_time": "2024-09-24T19:17:38.516066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa um dicionário para contar o número de acessos de cada agente de usuário\n",
    "UserAgents = {}\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer uma correspondência da linha com o padrão definido\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            # Se a linha corresponder ao padrão, armazena os grupos correspondentes em um dicionário\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']  # Extrai o agente de usuário da linha do log\n",
    "\n",
    "            # Atualiza a contagem do agente de usuário no dicionário\n",
    "            if agent in UserAgents:\n",
    "                UserAgents[agent] = UserAgents[agent] + 1  # Incrementa a contagem se já existir\n",
    "            else:\n",
    "                UserAgents[agent] = 1  # Inicializa a contagem em 1 se for a primeira vez\n",
    "\n",
    "# Ordena os agentes de usuário com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(UserAgents, key=lambda i: int(UserAgents[i]), reverse=True)\n",
    "\n",
    "# Imprime todos os agentes de usuário junto com suas contagens\n",
    "for result in results:\n",
    "    print(result + \": \" + str(UserAgents[result]))\n"
   ],
   "id": "31e6b885bd646495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0): 68484\n",
      "-: 4035\n",
      "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0): 1724\n",
      "W3 Total Cache/0.9.4.1: 468\n",
      "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html): 278\n",
      "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html): 248\n",
      "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36: 158\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0: 144\n",
      "Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H143 Safari/600.1.4: 120\n",
      "Mozilla/5.0 (Linux; Android 5.1.1; SM-G900T Build/LMY47X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36: 47\n",
      "Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm): 43\n",
      "Mozilla/5.0 (compatible; MJ12bot/v1.4.5; http://www.majestic12.co.uk/bot.php?+): 41\n",
      "Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14: 40\n",
      "Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots): 27\n",
      "Ruby: 15\n",
      "Mozilla/5.0 (Linux; Android 5.1.1; SM-G900T Build/LMY47X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.76 Mobile Safari/537.36: 15\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36: 13\n",
      "Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/): 11\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2: 11\n",
      "MobileSafari/600.1.4 CFNetwork/711.4.6 Darwin/14.0.0: 10\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.73 Safari/537.36: 9\n",
      "Mozilla/5.0 (compatible; YandexImages/3.0; +http://yandex.com/bots): 9\n",
      "Mozilla/5.0 (compatible; linkdexbot/2.0; +http://www.linkdex.com/bots/): 7\n",
      "Mozilla/5.0 (iPhone; CPU iPhone OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12F70 Safari/600.1.4 (compatible; Googlebot/2.1; +http://www.google.com/bot.html): 6\n",
      "Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp): 6\n",
      "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2.28) Gecko/20120306 Firefox/3.6.28 (.NET CLR 3.5.30729): 4\n",
      "Mozilla/5.0 zgrab/0.x: 4\n",
      "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36: 4\n",
      "Mozilla/5.0 (compatible; SeznamBot/3.2; +http://fulltext.sblog.cz/): 4\n",
      "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1): 4\n",
      "Mozilla/5.0: 3\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:34.0) Gecko/20100101 Firefox/34.0: 3\n",
      "Opera/9.80 (Windows NT 5.1; U; ru) Presto/2.9.168 Version/11.50: 3\n",
      "Mozilla/5.0 (compatible; spbot/4.4.2; +http://OpenLinkProfiler.org/bot ): 3\n",
      "Mozilla/4.0 (compatible: FDSE robot): 3\n",
      "Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1; 2Pac; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022): 3\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0: 3\n",
      "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:36.0) Gecko/20100101 Firefox/36.0: 2\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:36.0) Gecko/20100101 Firefox/36.0: 2\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:28.0) Gecko/20100101 Firefox/28.0: 2\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:2.0.1) Gecko/20100101 Firefox/5.0: 2\n",
      "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36: 2\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36: 2\n",
      "Googlebot-Image/1.0: 2\n",
      "netEstate NE Crawler (+http://www.website-datenbank.de/): 2\n",
      "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.5) Gecko/20041107 Firefox/1.0: 2\n",
      "Mozilla/5.0 (X11; U; FreeBSD x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16: 2\n",
      "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36: 2\n",
      "Opera/9.80 (Windows NT 6.1); U) Presto/2.10.289 Version/12.02: 2\n",
      "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 5.1; Trident/5.0; .NET CLR 2.0.50727; .NET CLR 3.5.30729): 2\n",
      "Mozilla/5.0 (Windows NT 6.2; rv:24.0) Gecko/20100101 Firefox/24.0: 2\n",
      "Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E): 2\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0: 2\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:28.0) Gecko/20100101 Firefox/28.0: 2\n",
      "Mozilla/5.0 (Windows NT 6.0; rv:29.0) Gecko/20120101 Firefox/29.0: 2\n",
      "Mozilla/5.0 (Windows NT 6.0; rv:31.0) Gecko/20100101 Firefox/31.0: 2\n",
      "Mozilla/5.0 (Windows NT 6.2; rv:31.0) Gecko/20100101 Firefox/31.0: 2\n",
      "Mozilla/4.0 (compatible; Netcraft Web Server Survey): 2\n",
      "Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; MAGWJS; rv:11.0) like Gecko: 1\n",
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36: 1\n",
      "Mozilla/5.0 (X11; U; Linux i686; pl-PL; rv:1.9.0.2) Gecko/20121223 Ubuntu/9.25 (jaunty) Firefox/3.8: 1\n",
      "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0: 1\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2251.0 Safari/537.36: 1\n",
      "Opera/9.80 (Windows NT 6.2; WOW64); U) Presto/2.12.388 Version/12.14: 1\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:33.0) Gecko/20100101 Firefox/33.0: 1\n",
      "Mozilla/5.0 (Windows; U; Windows NT 5.1; fr; rv:1.9.0.13) Gecko/2009073022 Firefox/3.0.13 (.NET CLR 3.5.30729): 1\n",
      "Scrapy/1.0.3 (+http://scrapy.org): 1\n",
      "Mozilla/4.0 (compatible; MSIE 6.0; MSIE 5.5; Windows 95) Opera 7.03 [de]: 1\n",
      "Telesphoreo: 1\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:31.0) Gecko/20100101 Firefox/31.0: 1\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.73 Safari/537.36: 1\n",
      "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.3.1.1) Gecko/20101203 Firefox/3.6.12 (.NET CLR 3.5.30309): 1\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36 Scanning for research (researchscan.comsys.rwth-aachen.de): 1\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:22.0) Gecko/20130405 Firefox/22.0: 1\n",
      "Mozilla/5.0 (Windows; U; Windows NT 5.0; fr-FR; rv:0.9.4) Gecko/20011019 Netscape6/6.2: 1\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:32.0) Gecko/20100101 Firefox/31.0: 1\n",
      "facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php): 1\n",
      "Mozilla/3.0 (compatible; Indy Library): 1\n",
      "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:42.0) Gecko/20100101 Firefox/42.0: 1\n",
      "Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0: 1\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.59.10 (KHTML, like Gecko) Version/5.1.9 Safari/534.59.10: 1\n",
      "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko: 1\n",
      "NokiaE5-00/SymbianOS/9.1 Series60/3.0 3gpp-gba: 1\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36: 1\n",
      "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36: 1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Caramba! Além de '-', há também um milhão de robôs web diferentes acessando o site e poluindo meus dados. Filtrar todos eles é realmente difícil, mas livrar-se daqueles que poluem significativamente meus dados, neste caso, deveria ser uma questão de se livrar de '-', qualquer coisa que contenha \"bot\" ou \"spider\" e W3 Total Cache.",
   "id": "fd8cb980191bbd7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:18:12.964924Z",
     "start_time": "2024-09-24T19:18:12.707314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa um dicionário para contar o número de acessos de cada URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer uma correspondência da linha com o padrão definido\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            # Se a linha corresponder ao padrão, armazena os grupos correspondentes em um dicionário\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']  # Extrai o agente de usuário da linha do log\n",
    "\n",
    "            # Verifica se o agente de usuário não é um bot ou spider\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent == '-')):\n",
    "                \n",
    "                # Extrai a requisição da linha do log\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                \n",
    "                # Verifica se a requisição contém 3 campos (método, URL e protocolo)\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    \n",
    "                    # Conta apenas requisições do tipo 'GET'\n",
    "                    if (action == 'GET'):\n",
    "                        # Incrementa a contagem da URL no dicionário\n",
    "                        if URL in URLCounts:\n",
    "                            URLCounts[URL] = URLCounts[URL] + 1  # Incrementa se já existir\n",
    "                        else:\n",
    "                            URLCounts[URL] = 1  # Inicializa em 1 se for a primeira vez\n",
    "\n",
    "# Ordena as URLs com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n"
   ],
   "id": "676c41af9011b01e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/?page_id=34248: 28\n",
      "/wp-content/cache/minify/000000/M9AvyUjVzUstLy7PLErVz8lMKkosqtTPKtYvTi7KLCgpBgA.js: 27\n",
      "/wp-content/cache/minify/000000/M9bPKixNLarUy00szs8D0Zl5AA.js: 27\n",
      "/wp-content/cache/minify/000000/lY7dDoIwDIVfiG0KxkfxfnbdKO4HuxICTy-it8Zw15PzfSftzPCckJem-x4qUWArqBPl5mygZLEgyhdOaoxToGyGaiALiOfUnIz0qDLOdSZGE-nOlpc3kopDzrSyavVVt_veb5qSDVhjsQ6dHh_B_eE_z2pYIGJ7iBWKeEio_eT9UQe4xHhDll27mGRryVu_pRc.js: 27\n",
      "/wp-content/cache/minify/000000/fY45DoAwDAQ_FMvkRQgFA5ZyWLajiN9zNHR0O83MRkyt-pIctqYFJPedKyYzfHg2PzOFiENAzaD07AxcpKmTolORvDjZt8KEfhBUGjZYCf8Fb0fvA1TXCw.css: 25\n",
      "/?author=1: 21\n",
      "/wp-content/cache/minify/000000/hcrRCYAwDAXAhXyEjiQ1YKAh4SVSx3cE7_uG7ASr4M9qg3kGWyk1adklK84LHtRj_My6Y0Pfqcz-AA.js: 20\n",
      "/wp-content/uploads/2014/11/nhn1.png: 19\n",
      "/wp-includes/js/wp-emoji-release.min.js?ver=4.3.1: 17\n",
      "/wp-content/cache/minify/000000/BcGBCQAgCATAiUSaKYSERPk3avzuht4SkBJnt4tHJdqgnPBqKldesTcN1R8.js: 17\n",
      "/wp-login.php: 16\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/favicon.ico: 10\n",
      "/wp-content/uploads/2014/11/violentcrime.jpg: 6\n",
      "/robots.txt: 6\n",
      "/wp-content/uploads/2014/11/garfield.jpg: 6\n",
      "/wp-content/uploads/2014/11/babyblues.jpg: 6\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agora, nosso novo problema é que estamos recebendo um monte de acessos a coisas que não são páginas da web. Não estamos interessados ​​neles, então vamos filtrar qualquer URL que não termine em / (todas as páginas do meu site são acessadas dessa maneira - novamente, isso é aplicar o conhecimento sobre meus dados à análise!)",
   "id": "3df85ba34bd546ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:19:08.852201Z",
     "start_time": "2024-09-24T19:19:08.566105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa um dicionário para contar o número de acessos de cada URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer uma correspondência da linha com o padrão definido\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            # Se a linha corresponder ao padrão, armazena os grupos correspondentes em um dicionário\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']  # Extrai o agente de usuário da linha do log\n",
    "\n",
    "            # Verifica se o agente de usuário não é um bot ou spider\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent == '-')):\n",
    "                \n",
    "                # Extrai a requisição da linha do log\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                \n",
    "                # Verifica se a requisição contém 3 campos (método, URL e protocolo)\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    \n",
    "                    # Checa se a URL termina com uma barra (\"/\")\n",
    "                    if (URL.endswith(\"/\")):\n",
    "                        # Conta apenas requisições do tipo 'GET'\n",
    "                        if (action == 'GET'):\n",
    "                            # Incrementa a contagem da URL no dicionário\n",
    "                            if URL in URLCounts:\n",
    "                                URLCounts[URL] = URLCounts[URL] + 1  # Incrementa se já existir\n",
    "                            else:\n",
    "                                URLCounts[URL] = 1  # Inicializa em 1 se for a primeira vez\n",
    "\n",
    "# Ordena as URLs com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n"
   ],
   "id": "de683a01adfbeeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/feed/: 2\n",
      "/sample-page/feed/: 2\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n",
      "/travel/feed/: 1\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Isso está começando a parecer mais verossímil! Mas se você fosse ainda mais fundo, descobriria que as páginas /feed/ são suspeitas e que alguns robôs ainda estão escapando. No entanto, é correto dizer que notícias de Orlando, notícias mundiais e quadrinhos são as páginas mais populares acessadas por um ser humano real neste dia.\n",
    "\n",
    "A moral da história é: conheça seus dados! E sempre questione e examine seus resultados antes de tomar decisões com base neles. Se sua empresa tomar uma decisão errada porque você forneceu uma análise de dados de origem incorretos, você poderá ter sérios problemas.\n",
    "\n",
    "Certifique-se de que as decisões tomadas durante a limpeza dos seus dados também sejam justificáveis ​​- não elimine os dados só porque eles não suportam os resultados desejados!"
   ],
   "id": "4884ce0f1b41b09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Activity\n",
    "Estes resultados ainda não são perfeitos; URLs que incluem “feed” não são, na verdade, páginas visualizadas por humanos. Modifique ainda mais este código para remover URLs que incluem \"/feed\". Melhor ainda, extraia algumas entradas de log dessas páginas e entenda de onde vêm essas visualizações."
   ],
   "id": "4c90c18df1665a6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:22:50.345245Z",
     "start_time": "2024-09-24T19:22:50.036394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Compilação do padrão de expressão regular para análise do log\n",
    "format_pat = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "def count_url_access(log_path):\n",
    "    # Inicializa um defaultdict para contar o número de acessos de cada URL\n",
    "    URLCounts = defaultdict(int)\n",
    "\n",
    "    # Definindo palavras-chave para bots\n",
    "    bot_keywords = {'bot', 'spider', 'W3 Total Cache', '-'}\n",
    "\n",
    "    # Abre o arquivo de log para leitura\n",
    "    with open(log_path, \"r\") as f:\n",
    "        for line in (l.rstrip() for l in f):\n",
    "            match = format_pat.match(line)\n",
    "            if match:\n",
    "                access = match.groupdict()\n",
    "                agent = access['user_agent']\n",
    "                \n",
    "                # Verifica se o agente de usuário é um bot ou spider\n",
    "                if not any(keyword in agent for keyword in bot_keywords):\n",
    "                    request = access['request']\n",
    "                    fields = request.split()\n",
    "                    \n",
    "                    # Verifica se a requisição contém 3 campos\n",
    "                    if len(fields) == 3:\n",
    "                        action, URL, protocol = fields\n",
    "                        \n",
    "                        # Checa se a URL termina com uma barra (\"/\") e se é uma requisição GET\n",
    "                        if URL.endswith(\"/\") and action == 'GET':\n",
    "                            URLCounts[URL] += 1  # Incrementa a contagem\n",
    "\n",
    "    # Ordena as URLs com base na contagem de acessos\n",
    "    results = sorted(URLCounts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "    for result in results[:20]:\n",
    "        print(f\"{result[0]}: {result[1]}\")\n",
    "\n",
    "# Chamada da função com o caminho do log\n",
    "count_url_access(logPath)\n"
   ],
   "id": "b37ebe35c92523fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 64\n",
      "/orlando-headlines/: 29\n",
      "/world/: 11\n",
      "/comics-2/: 10\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/feed/: 2\n",
      "/sample-page/feed/: 2\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n",
      "/travel/feed/: 1\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:22:25.965599Z",
     "start_time": "2024-09-24T19:22:25.671091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa um dicionário para contar o número de acessos de cada URL\n",
    "URLCounts = {}\n",
    "feed_entries = []  # Lista para armazenar entradas de log relacionadas a URLs que contêm \"/feed\"\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "\n",
    "            # Verifica se o agente de usuário não é um bot ou spider\n",
    "            if (not ('bot' in agent or 'spider' in agent or \n",
    "                      'Bot' in agent or 'Spider' in agent or\n",
    "                      'W3 Total Cache' in agent or agent == '-')):\n",
    "                \n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                \n",
    "                if len(fields) == 3:\n",
    "                    action, URL, protocol = fields\n",
    "                    \n",
    "                    # Checa se a URL termina com uma barra (\"/\")\n",
    "                    if URL.endswith(\"/\"):\n",
    "                        # Exclui URLs que contêm \"/feed\"\n",
    "                        if \"/feed\" in URL:\n",
    "                            feed_entries.append(line)  # Armazena a linha do log\n",
    "                            continue  # Ignora esta URL\n",
    "\n",
    "                        # Conta apenas requisições do tipo 'GET'\n",
    "                        if action == 'GET':\n",
    "                            URLCounts[URL] = URLCounts.get(URL, 0) + 1  # Incrementa ou inicializa\n",
    "\n",
    "# Ordena as URLs com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(URLCounts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "for result in results[:20]:\n",
    "    print(f\"{result[0]}: {result[1]}\")\n",
    "\n",
    "# Se houver entradas de log relacionadas a \"/feed\", imprime algumas delas\n",
    "if feed_entries:\n",
    "    print(\"\\nEntradas de log relacionadas a URLs que contêm '/feed':\")\n",
    "    for entry in feed_entries[:5]:  # Exibe as primeiras 5 entradas\n",
    "        print(entry)\n",
    "else:\n",
    "    print(\"\\nNenhuma entrada de log relacionada a URLs que contêm '/feed'.\")\n"
   ],
   "id": "6c9de4451c2ab499",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n",
      "\n",
      "Entradas de log relacionadas a URLs que contêm '/feed':\n",
      "195.154.250.88 - - [29/Nov/2015:22:35:35 +0000] \"GET /feed/ HTTP/1.1\" 200 17592 \"http://www.nohatenews.com/?feed=rss2\" \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)\"\n",
      "62.210.215.118 - - [01/Dec/2015:03:43:43 +0000] \"GET /sample-page/feed/ HTTP/1.1\" 500 200 \"-\" \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36\"\n",
      "62.210.215.118 - - [04/Dec/2015:16:34:59 +0000] \"GET /feed/ HTTP/1.1\" 200 4922 \"-\" \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36\"\n",
      "62.210.215.118 - - [04/Dec/2015:20:41:26 +0000] \"GET /sample-page/feed/ HTTP/1.1\" 500 200 \"-\" \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36\"\n",
      "62.210.215.118 - - [05/Dec/2015:21:34:28 +0000] \"GET /travel/feed/ HTTP/1.1\" 200 565 \"-\" \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36\"\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:25:23.249728Z",
     "start_time": "2024-09-24T19:25:22.987697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Inicializa um dicionário para contar o número de acessos de cada URL\n",
    "URLCounts = {}\n",
    "\n",
    "# Abre o arquivo de log para leitura\n",
    "with open(logPath, \"r\") as f:\n",
    "    # Itera sobre cada linha do arquivo, removendo espaços em branco à direita\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        # Tenta fazer uma correspondência da linha com o padrão definido\n",
    "        match = format_pat.match(line)\n",
    "        if match:\n",
    "            # Se a linha corresponder ao padrão, armazena os grupos correspondentes em um dicionário\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']  # Extrai o agente de usuário da linha do log\n",
    "            \n",
    "            # Verifica se o agente de usuário não é um bot ou spider, e não está vazio\n",
    "            if (not('bot' in agent.lower() or 'spider' in agent.lower() or \n",
    "                    'w3 total cache' in agent or agent == '-')):\n",
    "                \n",
    "                # Extrai a requisição da linha do log\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                \n",
    "                # Verifica se a requisição contém 3 campos (método, URL e protocolo)\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    \n",
    "                    # Checa se a URL termina com uma barra (\"/\") e não contém \"/feed\"\n",
    "                    if (URL.endswith(\"/\") and \"/feed\" not in URL):\n",
    "                        # Conta apenas requisições do tipo 'GET'\n",
    "                        if (action == 'GET'):\n",
    "                            # Incrementa a contagem da URL no dicionário\n",
    "                            if URL in URLCounts:\n",
    "                                URLCounts[URL] += 1  # Incrementa se já existir\n",
    "                            else:\n",
    "                                URLCounts[URL] = 1  # Inicializa em 1 se for a primeira vez\n",
    "\n",
    "# Ordena as URLs com base na contagem de acessos, do maior para o menor\n",
    "results = sorted(URLCounts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Imprime as 20 URLs mais acessadas junto com suas contagens\n",
    "for result in results[:20]:\n",
    "    print(f\"{result[0]}: {result[1]}\")\n"
   ],
   "id": "950f20c67b118d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f89ad2adc4109687"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
